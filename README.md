# Introduction to Natural Language Processing

## Dedicated to Solomon Marcus (100 years from his death)

# Chapter 1: Text Pre-Processing

- Tokenization
- Removal of Noise, URLs, Hashtag and User-mentions
- Word Segmentation
- Replacement of abbreviation and slang
- Replacing elongated characters
- Correction of Spelling mistakes
- Removing Numbers
- Lower-casing all words
- Removing Stop-words
- Lemmatization
- Part of Speech (POS) Tagging
- Handling Negations

# Chapter 2: Feature Extraction Methods

(a) Categorical Word Representation

- 1. One Hot Encodings
- 2. Bag of Words
     (b) Weighted Word Representation
- 1. Term Frequency
- 2. Term Frequency-Inverse Document Frequency
     (c) Continuos Word Representation
- 1. Word2Vec
     i. CBoW
     ii. Skip-gram
- 2. Gloval Verctors (GloVe)
- 3. FastText
     (c) Contextual Word Representation
- 1. Generic Context Word Representation (Context2Vec)
- 2. Contextualized word representations Vectors (CoVe)
- 3. Embedding from language Models (ELMo)
     (d) Transformer-based Pre-trained Language Models
- 1. GPT
- 2. Bidirectional Encoder Representations from Transformers (BERT)
- 3. Text-to Text Transfer Transformer (T5)
- 4. BART

# Chapter 3: Classification

- Naive Bayes
- Suport Vector Machine
- Linier Regresion
- Decision Tree
- Randow forest
- Deep Learning
  i. Recurent Neural Network (RNN)
  ii. Long Short Term Memory (LSTM)
  iii. Gated Recurent Unit (GRU)
  iv. Convolutional Neural Network (CNN)
